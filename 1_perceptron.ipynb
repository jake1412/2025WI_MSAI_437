{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2259f86-8b25-4ab4-b1d5-4e9397b5bac4",
   "metadata": {
    "id": "b2259f86-8b25-4ab4-b1d5-4e9397b5bac4"
   },
   "source": [
    "# Notebook 1: The Perceptron\n",
    "\n",
    "In this notebook, we'll explore the __perceptron__ model in the context of a binary classification task. We'll build a simple percepton model with NumPy and observe how it performs on a number of synthetic datasets.\n",
    "\n",
    "The notebook is broken up as follows:\n",
    "\n",
    "  1. <a href=\"#setup\">Setup</a>\n",
    "  2. [Simple Perceptron](#perceptron)\n",
    "  3. [Training and Testing on Data](#train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db07ea-b697-4536-b33e-b150ac1ad451",
   "metadata": {
    "id": "a7db07ea-b697-4536-b33e-b150ac1ad451",
    "tags": []
   },
   "source": [
    "## __1.__ <a name=\"setup\">Setup</a>\n",
    "\n",
    "We'll start by importing utility functions to create and plot simple two-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e9bdd-54b0-4dc1-85d6-c07346a5eeb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e5e9bdd-54b0-4dc1-85d6-c07346a5eeb0",
    "outputId": "7bfcadfa-c8fb-45a5-ec96-feda912cf42e"
   },
   "outputs": [],
   "source": [
    "# this \"magic\" command makes graphics produced by matplotlib appear in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np  # standard numerical processing library\n",
    "import torch  # This is PyTorch, what we're using for deep learning in class\n",
    "\n",
    "from utils.data import *  # see data.py for dataset utilities\n",
    "from utils.plotting import *  # see plotting.py for plotting utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f653d-352f-460c-8deb-847cf4b4b947",
   "metadata": {
    "id": "9f3f653d-352f-460c-8deb-847cf4b4b947"
   },
   "source": [
    "In a __binary classification__ task, our goal is to build a model capable of separating the positive and negative classes. Given an input, our model will predict a label (-1 or +1) corresponding to one of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bfc2f-50ae-42da-95d4-f880130cf5ad",
   "metadata": {
    "id": "c01bfc2f-50ae-42da-95d4-f880130cf5ad",
    "tags": []
   },
   "source": [
    "## __2.__ <a name=\"perceptron\">Simple Perceptron</a>\n",
    "\n",
    "We can train a linear (single-layer) perceptron to perform binary classification. A linear perceptron \"draws\" a hyperplane in the input space that separates the positive and negative classes, with points lying on each side of the hyperplane assigned to opposite classes.\n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"400px\" src=\"https://raw.githubusercontent.com/zachwooddoughty/cs449w23/main/notebooks/static/boundary.png\"/>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "How does the perceptron \"learn\" to draw a separating hyperplane? Like many other binary classification approaches (e.g. logistic regression), we can adjust a set of __weights__ that describe a linear boundary in the $d$-dimensional input space. For weights $\\mathbf{w} = \\{w_0, ..., w_d\\}$ and arbitrary input $\\mathbf{x} = \\{x_0, ..., x_d\\}$, we will define the perceptron's prediction as \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "      +1 & \\text{if}\\ \\mathbf{w}^T \\mathbf{x} \\geq 0 \\\\\n",
    "      -1 & \\text{if}\\ \\mathbf{w}^T \\mathbf{x} < 0 \\\\\n",
    " \\end{cases}  \n",
    "$$\n",
    "\n",
    "As shown in the figure above, it is common to include a __bias__ term; we can do so by simply extending the input dimension such that $\\mathbf{x} = \\{1, x_0, ..., x_d\\}$ and $\\mathbf{w} = \\{b, w_0, ..., w_d\\}$, meaning the extra weight is interpreted as a bias. Iterating over our dataset, we will tweak these weights based on our model's predictions until the model __converges__ or we reach an iteration limit.\n",
    "\n",
    "We can describe this process in pseudocode:\n",
    "\n",
    "  <pre>\n",
    "  <b>begin</b> initialize weights\n",
    "     <b>while</b> not converged or not exceeded maxEpochs\n",
    "          <b>for</b> each example in features\n",
    "              <b>if</b> example is misclassified using weights\n",
    "              <b>then</b> weights = weights + example * label_for_example * step_size\n",
    "     <b>return</b> weights\n",
    "  <b>end</b>\n",
    "  </pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565c3a2-84a2-4f5d-857f-1880969ae558",
   "metadata": {
    "id": "4565c3a2-84a2-4f5d-857f-1880969ae558"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.weights = None\n",
    "\n",
    "    def fit_single_epoch(self, data, labels, weights, step_size=1, verbose=False):\n",
    "        \"\"\"\n",
    "        Train a linear perceptron by iterating exactly once over the data.\n",
    "\n",
    "        PARAMETERS\n",
    "        ----------\n",
    "\n",
    "        data       A numpy array of real values with n rows and d columns, where n\n",
    "                   is the number of data points and d is the number of dimensions.\n",
    "\n",
    "        labels     A vector containing labels drawn from {1,-1}. The jth label\n",
    "                   is the label of the jth example data point in data.\n",
    "\n",
    "        weights    A vector of d+1 weights for a 1950's style perceptron, where d is\n",
    "                   the number of dimensions of the data\n",
    "\n",
    "        step_size  A positive real value in the range 0 < step_size <= 1\n",
    "\n",
    "        verbose    If true, it will print out its calculations after every\n",
    "                   looking at each data point\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        converged  Set to 1 if a line that separated the two classes was found.\n",
    "                   Else it is set to 0.\n",
    "\n",
    "        weights    A vector of d+1 weights that characterize the hyperplane that\n",
    "                   separates the data (if it convereged). If data has 3 dimensions,\n",
    "                   then weights will have 4 elements. The first element is the\n",
    "                   bias (aka offset), the remaining weights correspond to the weight\n",
    "                   associated with each of the 3 dimensions of the data.\n",
    "        \"\"\"\n",
    "\n",
    "        # To do the perceptron algorithm, add a column of ones to the data, these\n",
    "        # will correspond to the bias.\n",
    "        num_points, num_dimensions = np.shape(data)\n",
    "        b = np.ones(num_points)\n",
    "        bias = np.expand_dims(b, axis=1)\n",
    "        data_plus = np.append(bias, data, axis=1)\n",
    "\n",
    "        converged = 1\n",
    "        # we're being optimistic.\n",
    "\n",
    "        # iterate over every element in the data\n",
    "        for n in range(0, num_points):\n",
    "            point = data_plus[n]\n",
    "            label = labels[n]\n",
    "\n",
    "            # this is where the perceptron classifies\n",
    "            estimated_label = np.sign(np.sum(np.multiply(weights, point)))\n",
    "\n",
    "            if verbose:\n",
    "                print(\" point = \", point, \"  label = \", label, \" estimated_label = \", estimated_label)\n",
    "                print(\"old weights = \", np.round(weights, 3))\n",
    "\n",
    "            # This is where the perceptron updates\n",
    "            if estimated_label != label:\n",
    "                converged = 0  # admit we haven't converged\n",
    "                weights = weights + (step_size * label * point)  # change the weights of the hyperplane\n",
    "                if verbose:\n",
    "                    print(\"new weights = \", np.round(weights, 3))\n",
    "\n",
    "        return converged, weights\n",
    "\n",
    "    def fit(\n",
    "        self, data: np.ndarray, labels: np.ndarray, step_size: float = 1.0, max_iter: int = 10, verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train a linear perceptron by iterating over the data until an iteration\n",
    "        limit is reached or the model converges.\n",
    "\n",
    "        PARAMETERS\n",
    "        ----------\n",
    "\n",
    "        data       A numpy array of real values with n rows and d columns, where n\n",
    "                   is the number of data points and d is the number of dimensions.\n",
    "\n",
    "        labels     A vector containing labels drawn from {1,-1}. The jth label\n",
    "                   is the label of the jth example data point in data.\n",
    "\n",
    "        step_size  A positive real value in the range 0 < step_size <= 1\n",
    "\n",
    "        max_iter   The maximum number of iterations to perform\n",
    "\n",
    "        verbose    If true, it will print out its calculations after every\n",
    "                   looking at each data point\n",
    "        \"\"\"\n",
    "\n",
    "        # set the weights of the perceptron to an inital set of random values,\n",
    "        # and make sure to include \"extra\" bias weight\n",
    "        if self.weights is None:\n",
    "            n_features = data.shape[-1]\n",
    "            weights = np.random.rand(1, n_features + 1)\n",
    "        else:\n",
    "            weights = self.weights\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "\n",
    "            converged, weights = self.fit_single_epoch(data, labels, weights, step_size, verbose)\n",
    "\n",
    "            if converged:\n",
    "                break\n",
    "\n",
    "        # store learned weights\n",
    "        self.weights = weights\n",
    "\n",
    "    def predict(self, data: np.ndarray):\n",
    "        \"\"\"\n",
    "        This predicts a label (+1 or -1) for each example in the data and returns\n",
    "        predictions as a numpy array.\n",
    "\n",
    "        PARAMETERS\n",
    "        ----------\n",
    "\n",
    "        data     A numpy array of real values with n rows and d columns, where n\n",
    "                 is the number of data points and d is the number of dimensions.\n",
    "                 Each row is a data point to be classiified.\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        A vector with n labels drawn from (+1 or -1), where the ith\n",
    "        label is the predicted label of the ith data point\n",
    "        \"\"\"\n",
    "\n",
    "        # To do the perceptron algorithm, add a column of ones to the data, these\n",
    "        # will correspond to the bias.\n",
    "        num_points, num_dimensions = np.shape(data)\n",
    "        b = np.ones(num_points)\n",
    "        bias = np.expand_dims(b, axis=1)\n",
    "        data_plus = np.append(bias, data, axis=1)\n",
    "\n",
    "        # now predict classes\n",
    "        class_prediction = np.zeros(num_points)\n",
    "        for n in range(0, num_points):\n",
    "            point = data_plus[n]\n",
    "            class_prediction[n] = np.sign(np.sum(np.multiply(self.weights, point)))\n",
    "\n",
    "        return class_prediction\n",
    "\n",
    "    def __call__(self, data: np.ndarray):\n",
    "        return self.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f9172-9d9d-4170-b454-adda2ab84d80",
   "metadata": {
    "id": "322f9172-9d9d-4170-b454-adda2ab84d80"
   },
   "source": [
    "## __3.__ <a name=\"train\">Training and Testing on Data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e49729-c051-4052-b84c-3cd687ac9bd5",
   "metadata": {
    "id": "a1e49729-c051-4052-b84c-3cd687ac9bd5"
   },
   "source": [
    "Below, we'll try out four synthetic datasets:\n",
    "\n",
    "  * `make_two_gaussians_data` draws data points from a mixture of two Gaussians\n",
    "  * `make_center_surround_data` places data in two concentric circles\n",
    "  * `make_XOR_data` generates data representing the XOR boolean function\n",
    "  * `make_spiral_data` arranges the data in two spirals\n",
    "  \n",
    "In all cases, our data has two possible labels (-1 and +1) representing two classes. Let's have a look at an easy dataset to just see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e9078-0c3f-4cf8-950a-213678cdbbac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "097e9078-0c3f-4cf8-950a-213678cdbbac",
    "outputId": "3b93a1a5-efd9-4ee3-91f0-88075f702a62"
   },
   "outputs": [],
   "source": [
    "data, labels = make_two_gaussians_data(20, distance_between_means=2)\n",
    "plot_data(data, labels)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Wn5DKmAglz5",
   "metadata": {
    "id": "3Wn5DKmAglz5"
   },
   "source": [
    "We can now look at how our perceptron model performs when we train it on the various datasets. To show how the learning algorithm works in practice, we will visualize the model's learned decision boundary over a small number of training iterations. Note that the perceptron training algorithm _will only update weights when an incorrect prediction is made_. Therefore, the boundary may remain fixed for many iterations at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2c37b-e998-4427-924a-c41e126236dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "83c2c37b-e998-4427-924a-c41e126236dd",
    "outputId": "f91f9a7f-3e13-446c-af51-88f43277aad0"
   },
   "outputs": [],
   "source": [
    "# set the boundaries of decision plot\n",
    "data_min = np.min(np.min(data))\n",
    "data_max = np.max(np.max(data))\n",
    "num_points, num_dimensions = np.shape(data)\n",
    "\n",
    "# initialize model\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# train the model and visualize decision boundary, one step at a time\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    # select one data point\n",
    "    data_single_point, label_single_point = data[i : i + 1], labels[i : i + 1]\n",
    "\n",
    "    if i >= 1:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # plot the perceptron's current learned decision boundary\n",
    "        plot_decision_surface(perceptron, axis_limits=[data_min - 1, data_max + 1, data_min - 1, data_max + 1], ax=ax)\n",
    "\n",
    "        # visualize the data point\n",
    "        dummy = plt.text(data_single_point[:, 0] + 0.1, data_single_point[:, 1] + 0.1, \"current point\")\n",
    "        plot_data(data, labels, ax)\n",
    "        plt.gca().plot(data_single_point[:, 0], data_single_point[:, 1], \"k+\", markersize=14)\n",
    "\n",
    "        # look at the weights\n",
    "        title = \"step \" + str(i) + \", weights = \" + str(np.squeeze(np.round(perceptron.weights, 3)))\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    # fit the perceptron using the data point\n",
    "    perceptron.fit(data_single_point, label_single_point, step_size=0.2, max_iter=1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f0031-fd59-49f1-8883-9752c19c8126",
   "metadata": {
    "id": "747f0031-fd59-49f1-8883-9752c19c8126"
   },
   "source": [
    "In the plots above, we can see how the perceptron iteratively adjusts its weights in response to incorrect predictions. Now that we have a sense of how the perceptron model learns, let's go ahead an do a complete training run on all of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guGIXP-KwLSK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "guGIXP-KwLSK",
    "outputId": "3b95f0bf-cd52-4793-d2e4-d17dd08f4358"
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "datasets = {\n",
    "    \"two_gaussians\": make_two_gaussians_data(examples_per_class=100, distance_between_means=3),\n",
    "    \"center_surround\": make_center_surround_data(examples_per_class=100, distance_from_origin=5),\n",
    "    \"xor\": make_XOR_data(examples_per_class=100),\n",
    "    \"spiral\": make_spiral_data(examples_per_class=100),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(datasets), figsize=(16, 4))\n",
    "for idx, name in enumerate(datasets):\n",
    "    data, labels = datasets[name]\n",
    "    plot_data(data, labels, axes[idx])\n",
    "    axes[idx].set_title(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72cdd3-b42b-43af-8882-2d9138524d50",
   "metadata": {
    "id": "5e72cdd3-b42b-43af-8882-2d9138524d50"
   },
   "source": [
    "Uncomment the name of the dataset you'd like to train the perceptron on. Which of these datasets are  __linearly separable__ ? Is the perceptron capable of drawing anything other than a separating hyperplane in the input space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75798575-c285-4cc3-89aa-94979509d79f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "75798575-c285-4cc3-89aa-94979509d79f",
    "outputId": "7f4edb5b-25b1-4353-a258-8fef6604b4ff"
   },
   "outputs": [],
   "source": [
    "name = \"two_gaussians\"\n",
    "# name = \"center_surround\"\n",
    "# name = \"xor\"\n",
    "# name = \"spiral\"\n",
    "data, labels = datasets[name]\n",
    "\n",
    "# set the boundaries of decision plot\n",
    "data_min = np.min(np.min(data))\n",
    "data_max = np.max(np.max(data))\n",
    "num_points, num_dimensions = np.shape(data)\n",
    "\n",
    "# initialize model\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# plot the perceptron's iniial decision boundary\n",
    "perceptron.fit(data, labels, step_size=0.2, max_iter=0, verbose=False)\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Before training\")\n",
    "plot_decision_surface(perceptron, axis_limits=[data_min - 1, data_max + 1, data_min - 1, data_max + 1], ax=ax)\n",
    "plot_data(data, labels, ax)\n",
    "\n",
    "# fit the perceptron using the entire dataset\n",
    "perceptron.fit(data, labels, step_size=0.2, max_iter=1000, verbose=False)\n",
    "\n",
    "# plot the perceptron's learned decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"After training\")\n",
    "plot_decision_surface(perceptron, axis_limits=[data_min - 1, data_max + 1, data_min - 1, data_max + 1], ax=ax)\n",
    "plot_data(data, labels, ax)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "51afe244-9443-483f-8b79-772501a7f7d6",
    "168e8796-9bdb-4cf3-b8bc-be361c83b085",
    "17878cd1-3b52-4935-a1c4-488707561d6e"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

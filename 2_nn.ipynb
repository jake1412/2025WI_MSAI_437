{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b4f7ae8-9e3c-4543-8c6c-75be5b8bef5d",
   "metadata": {
    "id": "7b4f7ae8-9e3c-4543-8c6c-75be5b8bef5d"
   },
   "source": [
    "# Notebook 2: Neural Networks\n",
    "\n",
    "In this notebook, we'll build a __multi-layer perceptron__ (__neural network__) in PyTorch and demonstrate how we can train using automatic differentiation. We'll evaluate our model on a number of synthetic datasets for a simple binary classification task.\n",
    "\n",
    "The notebook is broken up as follows:\n",
    "\n",
    "  1. [Setup](#setup)  \n",
    "  2. [Multi-Layer Perceptron](#mlp)  \n",
    "  3. [PyTorch Essentials](#torch)  \n",
    "     3.1. [Defining a Model](#definition)  \n",
    "     3.2. [Backpropagation in a Nutshell](#backprop)  \n",
    "     3.3. [Optimizers](#opt)  \n",
    "  4. [Training a Model](#train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857219d-d2b1-4c38-8c7d-66b5bcfdd6a9",
   "metadata": {
    "id": "9857219d-d2b1-4c38-8c7d-66b5bcfdd6a9",
    "tags": []
   },
   "source": [
    "## __1.__ <a name=\"setup\">Setup</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lAHCqJevuxAO",
   "metadata": {
    "id": "lAHCqJevuxAO"
   },
   "source": [
    "Make sure the needed packages are installed and utility code is in the right place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pertAODYu7Ws",
   "metadata": {
    "id": "pertAODYu7Ws"
   },
   "source": [
    "Let's import utility functions to create and plot simple two-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd9b7d-c4da-46c4-98f9-52533749c31d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34dd9b7d-c4da-46c4-98f9-52533749c31d",
    "outputId": "c8ecaf71-4a70-449d-fe50-da11ae36edbe"
   },
   "outputs": [],
   "source": [
    "# this \"magic\" command makes graphics produced by matplotlib appear in the notebook.\n",
    "%matplotlib inline\n",
    "import numpy as np  # standard numerical processing library\n",
    "import torch  # This is PyTorch, what we're using for deep learning in class\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.data import *  # see data.py for dataset utilities\n",
    "from utils.plotting import *  # see plotting.py for plotting utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ac00e-4c4a-4e22-a70c-9cad80ec4441",
   "metadata": {
    "id": "bf0ac00e-4c4a-4e22-a70c-9cad80ec4441",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## __2.__ <a name=\"mlp\">Multi-Layer Perceptron</a>\n",
    "\n",
    "As we saw in the previous notebook, a fundamental limitation of the perceptron is its inability to model nonlinear decision functions. To give our perceptron the capacity to model more complex functions, we can make a couple of tweaks:\n",
    "\n",
    "1. We can combine multiple simple perceptron models in __layers__ to increase the number of learnable weights\n",
    "2. We can introduce __nonlinearities__ (nonlinear activation functions) between layers to allow our model to express complicated nonlinear relationships\n",
    "\n",
    "The resulting architecture is often called a __multi-layer perceptron__ or a __neural network__. As illustrated below, the combination of multiple weight layers and nonlinear activation functions is responsible for the multi-layer perceptron's increased expressivity.\n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img width=\"500px\" src=\"https://raw.githubusercontent.com/zachwooddoughty/cs449w23/main/notebooks/static/mlp.png\"/>\n",
    "</center>\n",
    "<br/>\n",
    "\n",
    "The PyTorch library allows us to easily define neural network models using built-in components such as layers and activation functions. In the remainder of this notebook, we'll cover the basics of how to build and train neural networks in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589572b-9a5b-401d-8fcb-6ef2b0b656af",
   "metadata": {
    "id": "9589572b-9a5b-401d-8fcb-6ef2b0b656af",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## __3.__ <a name=\"torch\">PyTorch Essentials</a>\n",
    "\n",
    "\n",
    "### __3.1.__ <a name=\"definition\">Defining A Model</a>\n",
    "\n",
    "Below, we implement the neural network shown above using built-in PyTorch components. Our network class inerits from `torch.nn.Module`, and in addition to a constructor must define a `forward()` method to specify how the model processes inputs into outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f07267-ecc9-47ab-853e-fdfc05a63cfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36f07267-ecc9-47ab-853e-fdfc05a63cfc",
    "outputId": "71d764fb-39b7-4677-a9b8-c071734fb1a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Our first neural network model. We'll keep it simple.\"\"\"\n",
    "\n",
    "    # This bit determines the architecture of each layer...but not how data flows\n",
    "    # Here we make 3 layers, which we named fc1, fc2 and fc3.\n",
    "    def __init__(self, activation=torch.tanh):\n",
    "        super().__init__()  # has to be here\n",
    "\n",
    "        self.layer1 = nn.Linear(2, 3)\n",
    "        self.layer2 = nn.Linear(3, 2)\n",
    "        self.layer3 = nn.Linear(2, 1)\n",
    "        self.activation = activation\n",
    "\n",
    "        for layer in [self.layer1, self.layer2, self.layer3]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    # This part determines how data in x flows through the network.\n",
    "    # Here, x is the input to the network.\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a7951-9d14-4ccc-bf4d-aa8785623774",
   "metadata": {},
   "source": [
    "* [Xavier initialization](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_) tries to maintain stable gradients through initialization.\n",
    "* Weights are drawn from a uniform distribution with bounds:\n",
    "  $$\\text{bound} = \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}$$\n",
    "  where:\n",
    "    - `fan_in`: The number of input neurons to the layer.\n",
    "    - `fan_out`: The number of output neurons from the layer.\n",
    "* By scaling the weights appropriately, gradients are less likely to shrink or grow uncontrollably during backpropagation.\n",
    "* Why uniform? The probability of picking any number within those bounds are equally likely.\n",
    "* For a layer defined as `nn.Linear(2, 3)`:\n",
    "    - Inputs (`fan_in`): 2\n",
    "    - Outputs (`fan_out`): 3\n",
    "    - Bound Calculation:\n",
    "  $$\\text{bound} = \\sqrt{\\frac{6}{2 + 3}} = \\sqrt{\\frac{6}{5}} \\approx 1.095 $$\n",
    "  The weights will be initialized uniformly in the range $$[-1.095, 1.095]$$\n",
    "* The sum of `fan_in` and `fan_out` balances how much variance is allowed in the weight initialization.\n",
    "* By ensuring weights are initialized symmetrically around zero, the network avoids bias in one direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e39392-25ec-4b8c-84ef-c79201cadea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model):\n",
    "    \"\"\"\n",
    "    A simple functon that prints out a PyTorch model's structural details\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model     a torch.nn.Model\n",
    "    \"\"\"\n",
    "\n",
    "    # Print model's state_dict\n",
    "    print(\"Model's state dictionary (stored weights):\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(\"  \", param_tensor, \"\\t\", tuple(model.state_dict()[param_tensor].size()))\n",
    "\n",
    "    # Print the number of parameters in the model\n",
    "    parameter_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"In total, this network has \", parameter_count, \" trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62399024-5801-49c5-9766-4e5fb6ea2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model and examine its weights\n",
    "net = Net()\n",
    "print_model(net)\n",
    "\n",
    "# given a 2-dimensional input...\n",
    "x = torch.randn(2)\n",
    "\n",
    "# ...the model produces 1-dimensional output\n",
    "output = net(x)\n",
    "print(f\"Input dimension: {x.shape}, Output dimension: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea154f59-859e-438b-8580-07a33b2e0e66",
   "metadata": {
    "id": "ea154f59-859e-438b-8580-07a33b2e0e66",
    "tags": []
   },
   "source": [
    "### __3.2.__ <a name=\"backprop\">Backpropagation in a Nutshell</a>\n",
    "\n",
    "Whereas our simple perceptron mapped two-dimensional inputs directly to scalar outputs, a neural network can map inputs through a number of __intermediate (\"hidden\") representations__ of varying dimensions. This additional complexity helps our model learn more difficult functions, but it also means we can no longer use the simple perceptron learning algorithm to train (i.e. we can no longer update our weights by directly adding incorrectly-predicted inputs). \n",
    "\n",
    "Luckily for us, the perceptron learning algorithm is just one of many optimization techniques that propagates an error measurement from the model's output to its weights in order to perform updates. When this error measurement is provided by a differentiable __loss function__ and our model's output is differentiable with respect to its weights, we can propagate error by computing the __gradient__ of the loss function with respect to each weight and performing a small update in the opposite direction of the gradient. The computation of these gradients is called __backpropagation__, and allows us to systematically train large and complex neural networks. \n",
    "\n",
    "\n",
    "If all of this sounds like a lot of work, don't worry -- PyTorch provides __automatic differentiation__, meaning that gradient computation is handled for us under-the-hood. Once a network is defined, training can be accomplished using a few function calls in a standard loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93ce36-6470-4afe-a130-e8517c78a83a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b93ce36-6470-4afe-a130-e8517c78a83a",
    "outputId": "8b57937e-3283-43de-9fe7-c37b0658924e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in PyTorch, all data (e.g. inputs, outputs, weights) is stored in Tensor objects.\n",
    "# Tensors act similarly to NumPy arrays, and share many methods\n",
    "x = torch.zeros(10)\n",
    "print(f\"Creating a tensor of type {type(x)} with shape {x.shape}\")\n",
    "print(f\"Starting x: {x}\")\n",
    "\n",
    "# During backpropagation, gradients will only be computed for tensors with the\n",
    "# `requires_grad` attribute set to True. We can set this manually if need be\n",
    "print(f\"Does our tensor require gradient computation? {x.requires_grad}\")\n",
    "x.requires_grad = True\n",
    "print(f\"Does our tensor require gradient computation? {x.requires_grad}\")\n",
    "\n",
    "# To perform backpropagation, we need to complete a \"forward pass\" in which\n",
    "# computations are performed on Tensor objects to compute a scalar loss value\n",
    "loss = 10 - x.sum()\n",
    "print(f\"Starting `loss` value: {loss}\")\n",
    "print(f\"Gradients of x: {x.grad}\")\n",
    "\n",
    "# PyTorch will compute all required gradients for tensors involved in the\n",
    "# computation of a scalar loss value once we call `.backward()`\n",
    "loss.backward()\n",
    "print(f\"Gradients of x: {x.grad}\")\n",
    "\n",
    "# We can update our `weights` in the opposite direction of this gradient to reduce our\n",
    "# loss value!\n",
    "x = x - x.grad\n",
    "print(f\"Updated x: {x.data}\")\n",
    "loss = 10 - x.sum()\n",
    "print(f\"Updated `loss` value: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13d34b-0580-4660-8b76-52d772d65146",
   "metadata": {
    "id": "6a13d34b-0580-4660-8b76-52d772d65146",
    "tags": []
   },
   "source": [
    "### __3.3__ <a name=\"opt\">Optimizers</a>\n",
    "\n",
    "In the above example we computed differentiable a scalar loss, used backpropagation to compute the gradients of the loss with respect to our \"weights,\" and performed a gradient-based update on our weights to reduce the loss. Rather than managing the weight-update process by hand, we can defer to a built-in __optimizer__ object that automatically adjusts weights based on stored gradients and standard hyperparameters (e.g. learning rate). When training neural networks with large numbers of parameters, this becomes much simpler than manually updating each weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c8528-af83-4175-9bf3-8f07b1694b67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c47c8528-af83-4175-9bf3-8f07b1694b67",
    "outputId": "a8863326-bb8d-4629-cbb3-4a091eb7674d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeat our simple optimization, this time using the optimizer\n",
    "x = torch.zeros(10).requires_grad_(True)\n",
    "print(f\"Starting x: {x}\")\n",
    "\n",
    "# create an optimizer object and pass it an Iterable containing our \"weights\"\n",
    "opt = torch.optim.SGD([x], lr=1.0)\n",
    "\n",
    "# compute loss and perform backpropagation\n",
    "loss = 10 - x.sum()\n",
    "loss.backward()\n",
    "\n",
    "# perform an optimization step, i.e. a gradient-based update of our weights\n",
    "opt.step()\n",
    "\n",
    "print(f\"Updated x: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a755881-96e2-4563-a8fc-11a857b8afe2",
   "metadata": {
    "id": "9a755881-96e2-4563-a8fc-11a857b8afe2",
    "tags": []
   },
   "source": [
    "## __4.__ <a name=\"train\">Training a Model</a>\n",
    "\n",
    "In the above examples, we \"trained\" a single tensor to minimize a loss function. Next, we'll use the same basic approach to train a neural network for a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777490a-7da8-4977-ac92-f6771c465823",
   "metadata": {
    "id": "4777490a-7da8-4977-ac92-f6771c465823"
   },
   "outputs": [],
   "source": [
    "def train_model(model, data, target):\n",
    "    \"\"\"\n",
    "    A simple functon that runs the network model through one epoch of the\n",
    "    training data and then updates the weights to move moodel output closer to the\n",
    "    target output.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model   A torch.nn.Model\n",
    "    data    A 2-D torch tensor where data[i] contains the ith example.\n",
    "    target  A torch tensor where target[i]  contains the expected output of the\n",
    "            model in response to input data[i]\n",
    "    \"\"\"\n",
    "\n",
    "    # This tells your model that you are in training mode, so that layers (like\n",
    "    # dropout, batchnorm) with different train-time and test-time behavior can\n",
    "    # behave accordingly. It doesn't do much on our simple model, but you need\n",
    "    # to do this as a standard practice!\n",
    "    model.train()\n",
    "\n",
    "    # Set the gradients to 0 before running the network on the data, so that\n",
    "    # loss gradients can be computed correctly during backpropagation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get the output of the network on the data\n",
    "    output = model(data)\n",
    "\n",
    "    # Measure the \"loss\" using mean squared error\n",
    "    loss = F.mse_loss(output.squeeze(), target)\n",
    "\n",
    "    # This calculates the gradients, performing backpropagation to propagate\n",
    "    # errors backward through the network's weights\n",
    "    loss.backward()\n",
    "\n",
    "    # This updates the network weights based on the freshly-computed gradient\n",
    "    # now stored alongside each weight\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test_model(model, data, target):\n",
    "    \"\"\"\n",
    "    A simple functon that prints out the model's loss and accuracy on the\n",
    "    data\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model     a torch.nn.Model\n",
    "    data      a 2-D torch tensor where data[i] contains the ith example.\n",
    "    target    a torch tensor where target[i]  contains the expected output\n",
    "            of the model in response to input data[i]\n",
    "    RETURNS\n",
    "    -------\n",
    "    accuracy  The accuracy on the data\n",
    "\n",
    "    loss      The loss on the data\n",
    "    \"\"\"\n",
    "\n",
    "    # This puts the network in evaluation mode so that layers (like\n",
    "    # dropout, batchnorm) with different train-time and test-time behavior can\n",
    "    # behave accordingly. It doesn't do much on our simple model, but you need\n",
    "    # to do this as a standard practice!\n",
    "    model.eval()\n",
    "\n",
    "    # torch.no_grad() tells the system to stop keeping track of gradients (derivatives)\n",
    "    # when performing calculations. This makes things run faster.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Get the output of the network on a batch of test examples\n",
    "        output = model(data)\n",
    "        output = output.squeeze()\n",
    "\n",
    "        # Measure loss\n",
    "        loss = F.mse_loss(output, target)\n",
    "\n",
    "        # Get the (thresholded) predictions of the network\n",
    "        prediction = torch.sign(output)\n",
    "\n",
    "        # Measure the accuracy of the predictions\n",
    "        c = prediction * target\n",
    "        accuracy = torch.mean((c + 1) / 2)\n",
    "\n",
    "        # lets report\n",
    "        # print('\\nTest set: loss = ', round(loss.item(),4),\n",
    "        #      ' accuracy = ', round(accuracy.item(),3), '\\n')\n",
    "        accuracy = round(accuracy.item(), 3)\n",
    "        loss = round(loss.item(), 4)\n",
    "\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LZ78PFarjdKg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "LZ78PFarjdKg",
    "outputId": "2812cca0-c8ba-47c1-8b73-86fee1d7fee8"
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "np.random.seed(3)\n",
    "datasets = {\n",
    "    \"two_gaussians\": make_two_gaussians_data(examples_per_class=30, distance_between_means=3),\n",
    "    \"center_surround\": make_center_surround_data(examples_per_class=30, distance_from_origin=5),\n",
    "    \"xor\": make_XOR_data(examples_per_class=30),\n",
    "    \"spiral\": make_spiral_data(examples_per_class=30),\n",
    "    \"noise\": make_noise_dataset(examples_per_class=10),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(datasets), figsize=(4 * len(datasets), 4))\n",
    "for idx, name in enumerate(datasets):\n",
    "    data, labels = datasets[name]\n",
    "    plot_data(data, labels, axes[idx])\n",
    "    axes[idx].set_title(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90400463-41f6-49b6-9306-41fe3d092725",
   "metadata": {
    "id": "90400463-41f6-49b6-9306-41fe3d092725"
   },
   "source": [
    "With our basic training and evaluation procedures defined, we can finally train a neural network! We'll start by loading converting our data into torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9336f-0815-4c3a-b5b5-87438bb47742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is in 64bit NumPy format, so we need to reformat the data as Tensors for PyTorch\n",
    "def dataset_to_tensors(name):\n",
    "    x_numpy, y_numpy = datasets[name]\n",
    "    x_torch = torch.tensor(x_numpy).to(dtype=torch.float32)\n",
    "    y_torch = torch.tensor(y_numpy).to(dtype=torch.float32)\n",
    "    return x_numpy, y_numpy, x_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fa9bf-9c11-4f98-bb38-229274007f1f",
   "metadata": {
    "id": "475fa9bf-9c11-4f98-bb38-229274007f1f"
   },
   "outputs": [],
   "source": [
    "x_numpy, y_numpy, x_torch, y_torch = dataset_to_tensors(\"two_gaussians\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025e7b7-b84d-42dc-a801-505a5875c509",
   "metadata": {
    "id": "7025e7b7-b84d-42dc-a801-505a5875c509"
   },
   "source": [
    "We can visualize the decision boundary drawn by our untrained model, as well as its initial accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787b652-9584-43c8-bf01-71ac03504693",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "7787b652-9584-43c8-bf01-71ac03504693",
    "outputId": "e9cc2be2-d3ee-4268-a90a-85c8dd5526f9"
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print_model(net)\n",
    "\n",
    "scale = 1\n",
    "\n",
    "# look at the decision surface and model decision surface\n",
    "plot_data(x_numpy, y_numpy)\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "plot_decision_surface(model=net, axis_limits=[xmin * scale, xmax * scale, ymin * scale, ymax * scale])\n",
    "\n",
    "# measure model accuracy\n",
    "accuracy, loss = test_model(net, x_torch, y_torch)\n",
    "print(\"accuracy=\" + str(accuracy) + \" loss=\" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c16650-605d-4cc4-ac8e-8e75c9c8c6d9",
   "metadata": {
    "id": "29c16650-605d-4cc4-ac8e-8e75c9c8c6d9"
   },
   "source": [
    "Finally, let's train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b3aa4-b1d6-4794-8bc0-4c357303b34f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "223b3aa4-b1d6-4794-8bc0-4c357303b34f",
    "outputId": "ee1acbb1-6b6b-4929-aa21-554b451da4a0"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "scale = 1\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for i in range(0, epochs):\n",
    "    train_model(net, x_torch, y_torch)\n",
    "\n",
    "plot_data(x_numpy, y_numpy)\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "plot_decision_surface(model=net, axis_limits=[xmin * scale, xmax * scale, ymin * scale, ymax * scale])\n",
    "accuracy, loss = test_model(net, x_torch, y_torch)\n",
    "print(\"accuracy=\" + str(accuracy) + \" loss=\" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee3445-9a24-4362-9a5d-7b0473152dc7",
   "metadata": {
    "id": "16ee3445-9a24-4362-9a5d-7b0473152dc7"
   },
   "source": [
    "As we can see, our simple neural network is capable of modeling nonlinear decision functions! Using the steps above, we can easily \"scale up\" our network to allow even more expressivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef93b0f-96de-4232-b926-b0b9f5c1b809",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "eef93b0f-96de-4232-b926-b0b9f5c1b809",
    "outputId": "f7aad87c-eade-4837-ae2f-54b3b857798d"
   },
   "outputs": [],
   "source": [
    "class BigNet(nn.Module):\n",
    "    \"\"\"Our second neural network model. We'll still keep it simple.\"\"\"\n",
    "\n",
    "    # this bit determines the architecture of each layer...but not how data flows\n",
    "    def __init__(self):\n",
    "        super().__init__()  # has to be here\n",
    "        self.fc1 = nn.Linear(2, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    # this part determines how data in x flows through the network.\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145fc09-cbce-4b81-a84c-99abd8863f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our model\n",
    "bignet = BigNet()\n",
    "optimizer = torch.optim.SGD(bignet.parameters(), lr=0.01, momentum=0.9)\n",
    "print_model(bignet)\n",
    "\n",
    "epochs = 1000\n",
    "scale = 1\n",
    "\n",
    "for i in range(0, epochs):\n",
    "    train_model(bignet, x_torch, y_torch)\n",
    "\n",
    "plot_data(x_numpy, y_numpy)\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "plot_decision_surface(model=bignet, axis_limits=[xmin * scale, xmax * scale, ymin * scale, ymax * scale])\n",
    "accuracy, loss = test_model(bignet, x_torch, y_torch)\n",
    "print(\"accuracy=\" + str(accuracy) + \" loss=\" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_SBFQURej1TO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "_SBFQURej1TO",
    "outputId": "8a249603-2bd1-4a03-be6b-33b32b9fec16"
   },
   "outputs": [],
   "source": [
    "name = \"two_gaussians\"\n",
    "# name = \"center_surround\"\n",
    "# name = \"xor\"\n",
    "# name = \"spiral\"\n",
    "# name = \"noise\"\n",
    "x_numpy, y_numpy, x_torch, y_torch = dataset_to_tensors(name)\n",
    "\n",
    "# set the boundaries of decision plot\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(8, 4))\n",
    "for ax in axes:\n",
    "    plot_data(x_numpy, y_numpy, ax=ax)\n",
    "    xmin, xmax, ymin, ymax = ax.axis()\n",
    "\n",
    "limits = [xmin * scale, xmax * scale, ymin * scale, ymax * scale]\n",
    "\n",
    "epochs = 1000\n",
    "scale = 1\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "for i in range(0, epochs):\n",
    "    train_model(net, x_torch, y_torch)\n",
    "\n",
    "plot_decision_surface(model=net, axis_limits=limits, ax=axes[0])\n",
    "accuracy, loss = test_model(net, x_torch, y_torch)\n",
    "axes[0].set_title(f\"Net: Acc={accuracy:.2f} Loss={loss:.2f}\")\n",
    "\n",
    "bignet = BigNet()\n",
    "optimizer = torch.optim.SGD(bignet.parameters(), lr=0.01, momentum=0.9)\n",
    "for i in range(0, epochs):\n",
    "    train_model(bignet, x_torch, y_torch)\n",
    "\n",
    "plot_decision_surface(model=bignet, axis_limits=limits, ax=axes[1])\n",
    "accuracy, loss = test_model(bignet, x_torch, y_torch)\n",
    "axes[1].set_title(f\"BigNet: Acc={accuracy:.2f} Loss={loss:.2f}\")\n",
    "axes[1].get_legend().remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A17_9XK0tBOz",
   "metadata": {
    "id": "A17_9XK0tBOz"
   },
   "outputs": [],
   "source": [
    "# Let's introduce a bit of experimental design\n",
    "def dataset_to_tensors_with_split(name):\n",
    "    x_numpy, y_numpy = datasets[name]\n",
    "\n",
    "    x_numpy_train, x_numpy_test, y_numpy_train, y_numpy_test = train_test_split(x_numpy, y_numpy, test_size=0.2, random_state=1)\n",
    "\n",
    "    x_torch_train = torch.tensor(x_numpy_train).to(dtype=torch.float32)\n",
    "    x_torch_test = torch.tensor(x_numpy_test).to(dtype=torch.float32)\n",
    "    y_torch_train = torch.tensor(y_numpy_train).to(dtype=torch.float32)\n",
    "    y_torch_test = torch.tensor(y_numpy_test).to(dtype=torch.float32)\n",
    "    return [\n",
    "        (x_numpy_train, x_numpy_test, y_numpy_train, y_numpy_test),\n",
    "        (x_torch_train, x_torch_test, y_torch_train, y_torch_test),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZkX02uexvJsQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "ZkX02uexvJsQ",
    "outputId": "147e82bc-a22b-44d4-e904-a966f0be0947"
   },
   "outputs": [],
   "source": [
    "# name = \"two_gaussians\"\n",
    "name = \"center_surround\"\n",
    "# name = \"xor\"\n",
    "# name = \"spiral\"\n",
    "# name = \"noise\"\n",
    "\n",
    "numpy_data, torch_data = dataset_to_tensors_with_split(name)\n",
    "(x_numpy_train, x_numpy_test, y_numpy_train, y_numpy_test) = numpy_data\n",
    "(x_torch_train, x_torch_test, y_torch_train, y_torch_test) = torch_data\n",
    "\n",
    "scale = 1\n",
    "limits = [\n",
    "    np.min(x_numpy_train[:, 0]) * scale - 1,\n",
    "    np.max(x_numpy_train[:, 0]) * scale + 1,\n",
    "    np.min(x_numpy_train[:, 1]) * scale - 1,\n",
    "    np.max(x_numpy_train[:, 1]) * scale + 1,\n",
    "]\n",
    "\n",
    "# Train both models on the training data\n",
    "epochs = 1000\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "for i in range(0, epochs):\n",
    "    train_model(net, x_torch_train, y_torch_train)\n",
    "\n",
    "bignet = BigNet()\n",
    "optimizer = torch.optim.SGD(bignet.parameters(), lr=0.01, momentum=0.9)\n",
    "for i in range(0, epochs):\n",
    "    train_model(bignet, x_torch_train, y_torch_train)\n",
    "\n",
    "# Create a grid of four plots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, sharey=True, sharex=True, figsize=(8, 8))\n",
    "\n",
    "# Top left: Net train\n",
    "axis = axes[0, 0]\n",
    "plot_data(x_numpy_train, y_numpy_train, ax=axis)\n",
    "plot_decision_surface(model=net, axis_limits=limits, ax=axis)\n",
    "accuracy, loss = test_model(net, x_torch_train, y_torch_train)\n",
    "axis.set_title(f\"Net Train: Acc={accuracy:.2f} Loss={loss:.2f}\")\n",
    "\n",
    "# Top right: Net test\n",
    "axis = axes[0, 1]\n",
    "plot_data(x_numpy_test, y_numpy_test, ax=axis)\n",
    "xmin, xmax, ymin, ymax = ax.axis()\n",
    "plot_decision_surface(model=net, axis_limits=limits, ax=axis)\n",
    "accuracy, loss = test_model(net, x_torch_test, y_torch_test)\n",
    "axis.set_title(f\"Net Test: Acc={accuracy:.2f} Loss={loss:.2f}\")\n",
    "\n",
    "# Bottom Left: BigNet train\n",
    "axis = axes[1, 0]\n",
    "plot_data(x_numpy_train, y_numpy_train, ax=axis)\n",
    "xmin, xmax, ymin, ymax = ax.axis()\n",
    "plot_decision_surface(model=bignet, axis_limits=limits, ax=axis)\n",
    "accuracy, loss = test_model(bignet, x_torch_train, y_torch_train)\n",
    "axis.set_title(f\"BigNet Train: Acc={accuracy:.2f} Loss={loss:.2f}\")\n",
    "\n",
    "# Bottom right: BigNet test\n",
    "axis = axes[1, 1]\n",
    "plot_data(x_numpy_test, y_numpy_test, ax=axis)\n",
    "xmin, xmax, ymin, ymax = ax.axis()\n",
    "plot_decision_surface(model=bignet, axis_limits=limits, ax=axis)\n",
    "accuracy, loss = test_model(bignet, x_torch_test, y_torch_test)\n",
    "axis.set_title(f\"BigNet Test: Acc={accuracy:.2f} Loss={loss:.2f}\")\n",
    "\n",
    "axes[1, 1].get_legend().remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946271d-9058-4cb8-8f8a-6d9e045e2fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
